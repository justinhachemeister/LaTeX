\documentclass[a4paper, 11pt]{article}
\usepackage{comment} % enables the use of multi-line comments (\ifx \fi) 
\usepackage{fullpage} % changes the margin
\usepackage[a4paper, total={7in, 10in}]{geometry}
\usepackage[fleqn]{amsmath,mathtools}
\usepackage{amssymb,amsthm}  % assumes amsmath package installed
\usepackage{float}
\usepackage{xcolor}
\usepackage{mdframed}
\usepackage[shortlabels]{enumitem}
\usepackage{indentfirst}
\usepackage{hyperref}
\hypersetup{
	colorlinks=true,
	linkcolor=blue,
	filecolor=magenta,      
	urlcolor=blue!70!red,
	pdftitle={Probability Assignment 3}, %%%%%%%%%%%%%%%%   WRITE ASSIGNMENT PDF NAME  %%%%%%%%%%%%%%%%%%%%
}
\usepackage[most,many,breakable]{tcolorbox}



\definecolor{mytheorembg}{HTML}{F2F2F9}
\definecolor{mytheoremfr}{HTML}{00007B}


\tcbuselibrary{theorems,skins,hooks}
\newtcbtheorem{problem}{Problem}
{%
	enhanced,
	breakable,
	colback = mytheorembg,
	frame hidden,
	boxrule = 0sp,
	borderline west = {2pt}{0pt}{mytheoremfr},
	sharp corners,
	detach title,
	before upper = \tcbtitle\par\smallskip,
	coltitle = mytheoremfr,
	fonttitle = \bfseries\sffamily,
	description font = \mdseries,
	separator sign none,
	segmentation style={solid, mytheoremfr},
}
{p}

% To give references for any problem use like this
% suppose the problem number is p3 then 2 options either 
% \hyperref[p:p3]{<text you want to use to hyperlink> \ref{p:p3}}
%                  or directly 
%                   \ref{p:p3}



\input{letterfonts}

\input{macros}

\setlength{\parindent}{0pt}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\textsf{\noindent \large\textbf{Soham Chatterjee} \hfill \textbf{Assignment - 3}\\
	Email: \href{sohamc@cmi.ac.in}{sohamc@cmi.ac.in} \hfill Roll: BMC202175\\
	\normalsize Course:
	%
	Probability
	%
	Theory \hfill Date: April 17, 2022 \\
	\noindent\rule{7in}{2.8pt}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Problem 1
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{problem}{%problem statemnet
}{p1% problem reference text
}
%Problem
Suppose that $X, Y$ are independent Poisson random variables with parameters $\lambda, \mu$.
\begin{enumerate}[label=(\alph*)]
	\item Show that $X+Y$ is also a Poisson variable with parameter $\lambda+\mu$.
	\item Show that the conditional distribution of $Y$ given $X+Y=n$ is a binomial random variable.
\end{enumerate}
\end{problem}

\solve{
	%Solution
	\begin{enumerate}[label=(\alph*)]
		\item Given that $X,Y$ are independent Poisson Random Variables with parameters $\lm.\mu$ respectively. Hence $$P(X=n)=e^{-\lm}\frac{\lm^n}{n!}\text{ and }P(Y=m)=e^{-\mu}\frac{\mu^m}{m!}$$.Now\begin{align*}
			      P(X+Y=k) & = \sum_{i=0}^k P(X=i,Y=k-i)                                                 \\
			               & = \sum_{i=0}^k P(X=i)P(Y=k-i)                                               \\
			               & = \sum_{i=0}^k e^{-\lm}\frac{\lm^i}{i!} \, e^{-\mu}\frac{\mu^{k-i}}{(k-i)!} \\
			               & = e^{-(\lm+\mu)}\sum_{i=0}^k\frac{\lm^i\mu^{k-i}}{i!(k-i)!}                 \\
			               & = e^{-(\lm+\mu)}\frac1{k!}\sum_{i=0}^k\frac{k!}{i!(k-i)!}\lm^i\mu^{k-i}     \\
			               & = e^{-(\lm+\mu)} \frac{(\lm+\mu)^k}{k!}
		      \end{align*}
		      Hence $X+Y$ is also a Poisson Random Variable.
		\item $X, Y$ are independent Poisson random variables with parameters $\lambda, \mu$. Hence by the part (a) $X+Y$ is also a Poisson Random Variable. To find the conditional distribution of $Y$ given $X+Y=n$ let $Z=X+Y$ then the parameter of $Z$ is $\lm+\mu$. Let $f\st_{{} Y\mid Z}(k)=P(Y=k\mid X+Y=n)$. Hence \begin{align*}
			      f\st_{Y\mid Z}(k\mid n) & =P(Y=k\mid X+Y=n)                                                                                              \\
			                              & =\frac{P(Y=k,X+Y=n)}{P(X+Y=n)}                                                                                 \\
			                              & =\frac{P(X=n-k)P(Y=k)}{P(X+Y=n)}                                                                               \\
			                              & =\frac{e^{-\lm}\dfrac{\lm^{n-k}}{(n-k)!}\, e^{-\mu}\dfrac{\mu^{k}}{k!}}{e^{-(\lm+\mu)}\dfrac{(\lm+\mu)^n}{n!}} \\
			                              & =\frac{n!}{k!(n-k)!}\, \frac{\lm^{n-k}}{(\lm+\mu)^{n-k}}\, \frac{\mu^k}{(\lm+\mu)^k}                           \\
			                              & =\binom{n}{k} \lt(\frac{\mu}{\lm+\mu}\rt)^k\lt(1-\frac{\mu}{\lm+\mu}\rt)^{n-k}
		      \end{align*}
		      Hence $f\st_{Y\mid Z}$ is a Binomial Random Variable $Bin\lt(n,\frac{\mu}{\lm+\mu}\rt)$. Therefore the conditional distribution of $Y$ given $X+Y=n$ is a binomial random variable.
	\end{enumerate}
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Problem 2
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{problem}{%problem statemnet
}{p2% problem reference text
}
%Problem		
\begin{enumerate}[label=(\alph*)]
	\item Show that $Z=X+Y$ is a binomial random variable if $X, Y$ are independent binomial variables $Bin(n, p), Bin(m, p)$.
	\item The hypergeometric random with parameters $N, n, k$ is the random variable with mass function $f(x)={\binom{k}{x}\binom{N-k}{n-x}/\binom{N}{n}},\ x \in \mathbb{Z}_{\geq 0}$. Show that if $X, Y$ are independent binomial variables $B(n, p)$ and if $Z=X+Y$, then $X \mid Z$ is hypergeometric.
\end{enumerate}
\end{problem}

\solve{
	%Solution
	\begin{enumerate}[label=(\alph*)]
		\item $X,Y$ are independent Binomial Random Variables $Bin(n,p),Bin(m,p)$ respectively. Hence $$f\st+X(x)=P(X=x)=\binom{n}{x}p^x(1-p)^{n-x} \text{ and }f\st_Y(y)=P(Y=y)\binom{n}{y}p^y(1-p)^{n-y}$$Now let $Z=X+Y$. Then \begin{align*}
			      f\st_Z(z) & =P(Z=z)                                                                     \\
			                & =\sum_{k=0}^zP(X=k,Y=z-k)                                                   \\
			                & =\sum_{k=0}^zP(X=k)P(Y=z-k)                                                 \\
			                & =\sum_{k=0}^z\binom{n}{k}p^k(1-p)^{n-k}\binom{m}{z-k}p^{z-k}(1-p)^{m-(z-k)} \\
			                & =p^{z}(1-p)^{n+m-z}\sum_{k=0}^z\binom{n}{k}\binom{m}{z-k}                   \\
			                & =\binom{n+m}{z}p^z(1-p)^{n+m-z}
		      \end{align*}
		      Hence $Z$ is a Binomial Random Variable $Bin(n+m,p)$
		\item $X,Y$ are independent Binomial Random Variables $B(n,p)$ and $Z=X+Y$. Hence$$f\st_X(x)=P(X=x)=\binom{n}{x}p^x(1-p)^{n-x}\text{ and }f\st_Y(y)=P(Y=y)=\binom{n}{y}p^y(1-p)^{n-y}$$ $$f\st_Z(z)=P(Z=z)=\binom{2n}{z}p^z(1-p)^{2n-z}$$Now \begin{align*}
			      f\st_{X\mid Z}(x\mid z) & =P(X=x\mid Z=z)                \\
			                              & =\frac{P(X=x,Z=z)}{P(Z=z)}     \\
			                              & =\frac{P(X=x,Y=z-x)}{P(Z=z)}   \\
			                              & =\frac{P(X=x)P(Y=z-x)}{P(Z=z)}
		      \end{align*}
		      \begin{align*}
			      \qquad\qquad\quad & =\frac{\displaystyle{\binom{n}{x}p^x(1-p)^{n-x}\binom{n}{z-x}p^{z-x}(1-p)^{n-(z-x)}}}{\displaystyle{\binom{2n}{z}p^z(1-p)^{2n-z}}}                                \\
			                        & =\frac{\displaystyle{\binom{n}{x}\binom{n}{z-x}}}{\displaystyle{\binom{2n}{z}}}=\frac{\displaystyle{\binom{n}{x}\binom{2n-n}{z-x}}}{\displaystyle{\binom{2n}{z}}}
		      \end{align*}Hence $f\st_{X\mid Z}=f(x)$ where $f$ is the mass function of hypergeometric random variable with parameters $2n,z,n$. Hence $X\mid Z$ is hypergeometric
	\end{enumerate}
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Problem 3
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{problem}{%problem statemnet
}{p3% problem reference text
}
%Problem		
Let $X=\left(X_{1}, \ldots, X_{n}\right)$ be a vector of (discrete) random variables. Assume that $\operatorname{cov}\left(X_{i} X_{j}\right)$ exists for all $i, j$. The \textit{covariance matrix} $V(X)$ of $X$ is defined as the $n \times n$ matrix $\left(v_{i, j}\right)$ where $v_{i, j}=\operatorname{cov}\left(X_{i}, X_{j}\right)$. Show that $\operatorname{det}(V(X))=0$ if and only if there exist $a_{0}, a_{1}, \ldots, a_{n} \in \mathbb{R}$ such that $P\left(\sum\limits_{1 \leq j \leq n} a_{j} X_{j}=a_{0}\right)=1$.
\end{problem}

\solve{
	%Solution
	For this we will first prove two lemmas
	\vspace*{5mm}
	\parinf

	\textbf{\textit{Lemma 1: }}If $X,Y$  are two random variables and $a\in\bbR$ then $aCov(X,Y)=Cov(aX,Y)=Cov(X,aY).$

	\textbf{\textit{Proof: }}$Cov(X,Y)=E((X-E(X))(Y-E(Y))$. Hence\begin{multline*}
		aCov(X,Y)=aE((X-E(X))(Y-E(Y))=E(a(X-E(X))(Y-E(Y))\\
		=E((aX-E(aX))(Y-E(Y))=Cov(aX,Y)
	\end{multline*}Similarly\begin{multline*}
		aCov(X,Y)=aE((X-E(X))(Y-E(Y))=E((X-E(X))a(Y-E(Y))\\
		=E((X-E(X))(aY-E(aY))=Cov(X,aY)
	\end{multline*}Therefore $aCov(X,Y)=Cov(aX,Y)=Cov(X,aY)$

	\textbf{\textit{Lemma 2: }} Let $X$ is a discrete random variable. If $Var(X)=0$ then  $P(X=E(X))=1$ and if there exists a real number $a$ such that $P(x=a)=1$ then $a=E(X)$ and $Var(X)=0$

	\textbf{\textit{Proof: }}\parinn If $E(X^2)=0$ then $\sum\limits_{x}x^2P(X=x)=0$ for all $x\in S$ where $S$ is the set of all values the random variable takes. Since for all $x\in S$, $x^2P(X=x)\geq 0$ we can say $P(X=x)=0$ for all $x\in S$, $x\neq 0$. Hence $$P(X=0)=1-\sum_{x\neq 0}x^2P(x)=1-0=1$$. Hence $P(X=E(X)=0)=1$. Now let's assume $Var(X)=0$. Then $$Var(X)=0\implies E((X-E(X))^2)=0\implies P(X=E(X))=1$$

	Now let there exists a real number $a$ such that $P(X=a)=1$. Then $P(X=x)=0$ where $\forall \ x\neq a$. Hence $$E(X)=\sum_{x}xP(X=x)=aP(X=a)=a$$Hence $a=E(X)$ proved. Now $$Var(X)=E(X^2)-E(X)^2=E(X^2)-a^2=\sum_{x}x^2P(X=x)-a^2=a^2P(X=a)-a^2=a^2-a^2=0$$Hence $Var(X)=0$ proved\parinf


	\textbf{\textit{Lemma 3: }}Covariance Matrix is positive semidefinite

	\textbf{\textit{Proof: }}We have $v_{ij}=Cov(X_i,X_j)=E\lt[(X_i-E(X_i))(X_j-E(X_j))\rt]$. Let $Y=\begin{bmatrix}y_1 & y_2 & \cdots & y_n\end{bmatrix}^T$ be a real vector where each $y_i\in \bbR$. Now\begin{align*}Y^TV(X)Y & = \begin{bmatrix}y_1 & y_2 & \cdots & y_n\end{bmatrix}\begin{bmatrix}v_{11} & \cdots & v_{1n}\\	\vdots & \ddots & \vdots\\	v_{n1} & \cdots & v_{nn}\end{bmatrix}\begin{bmatrix}y_1\\ y_2 \\ \vdots \\y_n\end{bmatrix} \\
                       & =\sum_{1\leq i,j\leq n} y_iy_iCov(X_i,X_j)                                                                                                                                                                                                      \\
                       & =\sum_{1\leq i,j\leq n}Cov(y_iX_i,y_jX_j)\qquad [\text{Using }Lemma\ 1]                                                                                                                                                                         \\
                       & =\sum_{1\leq i,j\leq n}Cov(Z_i,Z_j)\qquad [\text{Let }Z_i=y_iX_i]                                                                                                                                                                               \\
                       & =\sum_{1\leq i,j\leq n}E\lt((Z_i-E(Z_i))(Z_j-E(Z_j))\rt)                                                                                                                                                                                        \\
                       & =E\lt(\sum_{1\leq i,j\leq n}(Z_i-E(Z_i))(Z_j-E(Z_j))\rt)                                                                                                                                                                                        \\
                       & = E\lt(\lt(\sum_{i=1}^n(Z_i-E(Z_i))\rt)^2\rt)                                                                                                                                                                                                   \\
                       & =E\lt[\lt[\lt(\sum_{i=1}^nZ_i\rt)-\lt(\sum_{i=1}^nE(Z_i)\rt)\rt]^2\rt]                                                                                                                                                                          \\
                       & =E\lt[\lt[\lt(\sum_{i=1}^nZ_i\rt)-E\lt(\sum_{i=1}^nZ_i\rt)\rt]^2\rt]
	\end{align*}Now for a random discrete random variable, $U$. Now $E(U^2)=\sum\limits_{u} u^2P(U=u)$ where $u^2\geq 0, P(U=u)\geq 0$ hence $E(U^2)\geq 0$ Therefore $Y^TV(X)Y\geq 0$ and hence covariance matrix is positive semidefinite.

	\textbf{\textit{Lemma 4: }}$\det(V(X))=0\iff\exs\ u\in\bbR^n\setminus\{0\}$ s.t $u^TV(X)u=0$

	\textbf{\textit{Proof: }}$\det(V(X))=0\iff $$\exs$ a vector $Y$ such that $V(X)Y=0$. Let $Y=\begin{bmatrix}
		y_1 & y_2 & \cdots & y_n
	\end{bmatrix}^T$ and $V(X)Y=0\iff Y^TV(X)Y=0$. Now $$Y^TV(X)Y=0\iff E\lt[\lt[\lt(\sum_{i=1}^nZ_i\rt)-E\lt(\sum_{i=1}^nZ_i\rt)\rt]^2\rt]  =0\iff Var\lt(\sum_{i=1}^nZ_i\rt)=0$$\parinn

	Now for the backward direction. There exists a vector $Y\in \bbR^n\setminus\{0\}$ such that $Y^TV(X)Y=0$. By \textit{Lemma 3} $V(X)$ is positive semidefinite. hence all of eigen values of $V(X)$ are nonnegative. If all eigen values are positive then $V(X)$ is positive definite. Then $\forall\ Y\in\bbR^n\setminus\{0\}, $ $Y^TV(X)Y>0$. This contradicts our assumption that there exist a vector $u\in\bbR^n\setminus\{0\}$ such that $u^TV(X)u=0$. Hence contradiction. Therefore at least one eigen value is 0. Hence $\det(V(X))=0$

	\parinn
	\vspace{5mm}

	Now coming back to the solution of for the covariance matrix $V(X)$,  $\det(V(X))=0\iff\exs\ Y\in\bbR^n\setminus\{0\}$ s.t $Y^TV(X)Y=0$ where $Y=\begin{bmatrix}
			y_1 & y_2 & \cdots & y_n		\end{bmatrix}^T$  Let $Z=\begin{bmatrix}
			y_1X_1 & y_2X_2 & \cdots & y_nX_n
		\end{bmatrix}^T$.                     Hence \begin{multline*}
		\det(V(X))=0\iff Var\lt(\sum\limits_{i=1}^nZ_i\rt)=0\xLeftrightarrow{\text{By \textit{Lemma 2}}} P\lt(\sum\limits_{i=1}^n a_iX_i = E\lt(\sum\limits_{i=1}^n a_iX_i\rt)\rt)=1\\\iff\exs\ a_0,a_1,a_2,\dots,a_n\in\bbR\text{ such that }P\lt(\sum_{i=1}^na_iX_i=a_0\rt)=1
	\end{multline*}
}
\end{document}
